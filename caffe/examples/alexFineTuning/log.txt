I1115 10:57:02.820422 18097 caffe.cpp:217] Using GPUs 0
I1115 10:57:02.837671 18097 caffe.cpp:222] GPU 0: GeForce GTX TITAN X
I1115 10:57:03.103773 18097 solver.cpp:48] Initializing solver from parameters: 
test_iter: 30
test_interval: 200
base_lr: 0.005
display: 500
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 10000
snapshot_prefix: "examples/alexFineTuning/alexFineTuning_train"
solver_mode: GPU
device_id: 0
net: "examples/alexFineTuning/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1115 10:57:03.103900 18097 solver.cpp:91] Creating training net from net file: examples/alexFineTuning/train_val.prototxt
I1115 10:57:03.104470 18097 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1115 10:57:03.104496 18097 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1115 10:57:03.104682 18097 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "examples/alexFineTuning/mean.binaryproto"
  }
  data_param {
    source: "examples/alexFineTuning/img_train_lmdb"
    batch_size: 200
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1115 10:57:03.104781 18097 layer_factory.hpp:77] Creating layer data
I1115 10:57:03.105240 18097 net.cpp:100] Creating Layer data
I1115 10:57:03.105252 18097 net.cpp:408] data -> data
I1115 10:57:03.105269 18097 net.cpp:408] data -> label
I1115 10:57:03.105279 18097 data_transformer.cpp:25] Loading mean file from: examples/alexFineTuning/mean.binaryproto
I1115 10:57:03.106858 18102 db_lmdb.cpp:35] Opened lmdb examples/alexFineTuning/img_train_lmdb
I1115 10:57:03.127099 18097 data_layer.cpp:41] output data size: 200,3,227,227
I1115 10:57:03.299327 18097 net.cpp:150] Setting up data
I1115 10:57:03.299363 18097 net.cpp:157] Top shape: 200 3 227 227 (30917400)
I1115 10:57:03.299367 18097 net.cpp:157] Top shape: 200 (200)
I1115 10:57:03.299371 18097 net.cpp:165] Memory required for data: 123670400
I1115 10:57:03.299378 18097 layer_factory.hpp:77] Creating layer conv1
I1115 10:57:03.299399 18097 net.cpp:100] Creating Layer conv1
I1115 10:57:03.299407 18097 net.cpp:434] conv1 <- data
I1115 10:57:03.299417 18097 net.cpp:408] conv1 -> conv1
I1115 10:57:03.301334 18097 net.cpp:150] Setting up conv1
I1115 10:57:03.301349 18097 net.cpp:157] Top shape: 200 96 55 55 (58080000)
I1115 10:57:03.301352 18097 net.cpp:165] Memory required for data: 355990400
I1115 10:57:03.301363 18097 layer_factory.hpp:77] Creating layer relu1
I1115 10:57:03.301370 18097 net.cpp:100] Creating Layer relu1
I1115 10:57:03.301373 18097 net.cpp:434] relu1 <- conv1
I1115 10:57:03.301378 18097 net.cpp:395] relu1 -> conv1 (in-place)
I1115 10:57:03.301385 18097 net.cpp:150] Setting up relu1
I1115 10:57:03.301393 18097 net.cpp:157] Top shape: 200 96 55 55 (58080000)
I1115 10:57:03.301395 18097 net.cpp:165] Memory required for data: 588310400
I1115 10:57:03.301398 18097 layer_factory.hpp:77] Creating layer pool1
I1115 10:57:03.301403 18097 net.cpp:100] Creating Layer pool1
I1115 10:57:03.301409 18097 net.cpp:434] pool1 <- conv1
I1115 10:57:03.301412 18097 net.cpp:408] pool1 -> pool1
I1115 10:57:03.301484 18097 net.cpp:150] Setting up pool1
I1115 10:57:03.301492 18097 net.cpp:157] Top shape: 200 96 27 27 (13996800)
I1115 10:57:03.301496 18097 net.cpp:165] Memory required for data: 644297600
I1115 10:57:03.301497 18097 layer_factory.hpp:77] Creating layer norm1
I1115 10:57:03.301517 18097 net.cpp:100] Creating Layer norm1
I1115 10:57:03.301534 18097 net.cpp:434] norm1 <- pool1
I1115 10:57:03.301540 18097 net.cpp:408] norm1 -> norm1
I1115 10:57:03.320410 18097 net.cpp:150] Setting up norm1
I1115 10:57:03.320422 18097 net.cpp:157] Top shape: 200 96 27 27 (13996800)
I1115 10:57:03.320425 18097 net.cpp:165] Memory required for data: 700284800
I1115 10:57:03.320427 18097 layer_factory.hpp:77] Creating layer conv2
I1115 10:57:03.320444 18097 net.cpp:100] Creating Layer conv2
I1115 10:57:03.320447 18097 net.cpp:434] conv2 <- norm1
I1115 10:57:03.320452 18097 net.cpp:408] conv2 -> conv2
I1115 10:57:03.328284 18097 net.cpp:150] Setting up conv2
I1115 10:57:03.328305 18097 net.cpp:157] Top shape: 200 256 27 27 (37324800)
I1115 10:57:03.328308 18097 net.cpp:165] Memory required for data: 849584000
I1115 10:57:03.328318 18097 layer_factory.hpp:77] Creating layer relu2
I1115 10:57:03.328326 18097 net.cpp:100] Creating Layer relu2
I1115 10:57:03.328330 18097 net.cpp:434] relu2 <- conv2
I1115 10:57:03.328336 18097 net.cpp:395] relu2 -> conv2 (in-place)
I1115 10:57:03.328346 18097 net.cpp:150] Setting up relu2
I1115 10:57:03.328351 18097 net.cpp:157] Top shape: 200 256 27 27 (37324800)
I1115 10:57:03.328354 18097 net.cpp:165] Memory required for data: 998883200
I1115 10:57:03.328356 18097 layer_factory.hpp:77] Creating layer pool2
I1115 10:57:03.328362 18097 net.cpp:100] Creating Layer pool2
I1115 10:57:03.328367 18097 net.cpp:434] pool2 <- conv2
I1115 10:57:03.328372 18097 net.cpp:408] pool2 -> pool2
I1115 10:57:03.328402 18097 net.cpp:150] Setting up pool2
I1115 10:57:03.328408 18097 net.cpp:157] Top shape: 200 256 13 13 (8652800)
I1115 10:57:03.328410 18097 net.cpp:165] Memory required for data: 1033494400
I1115 10:57:03.328413 18097 layer_factory.hpp:77] Creating layer norm2
I1115 10:57:03.328419 18097 net.cpp:100] Creating Layer norm2
I1115 10:57:03.328423 18097 net.cpp:434] norm2 <- pool2
I1115 10:57:03.328428 18097 net.cpp:408] norm2 -> norm2
I1115 10:57:03.328449 18097 net.cpp:150] Setting up norm2
I1115 10:57:03.328455 18097 net.cpp:157] Top shape: 200 256 13 13 (8652800)
I1115 10:57:03.328457 18097 net.cpp:165] Memory required for data: 1068105600
I1115 10:57:03.328460 18097 layer_factory.hpp:77] Creating layer conv3
I1115 10:57:03.328469 18097 net.cpp:100] Creating Layer conv3
I1115 10:57:03.328472 18097 net.cpp:434] conv3 <- norm2
I1115 10:57:03.328476 18097 net.cpp:408] conv3 -> conv3
I1115 10:57:03.349167 18097 net.cpp:150] Setting up conv3
I1115 10:57:03.349195 18097 net.cpp:157] Top shape: 200 384 13 13 (12979200)
I1115 10:57:03.349198 18097 net.cpp:165] Memory required for data: 1120022400
I1115 10:57:03.349210 18097 layer_factory.hpp:77] Creating layer relu3
I1115 10:57:03.349220 18097 net.cpp:100] Creating Layer relu3
I1115 10:57:03.349222 18097 net.cpp:434] relu3 <- conv3
I1115 10:57:03.349227 18097 net.cpp:395] relu3 -> conv3 (in-place)
I1115 10:57:03.349236 18097 net.cpp:150] Setting up relu3
I1115 10:57:03.349238 18097 net.cpp:157] Top shape: 200 384 13 13 (12979200)
I1115 10:57:03.349241 18097 net.cpp:165] Memory required for data: 1171939200
I1115 10:57:03.349243 18097 layer_factory.hpp:77] Creating layer conv4
I1115 10:57:03.349251 18097 net.cpp:100] Creating Layer conv4
I1115 10:57:03.349253 18097 net.cpp:434] conv4 <- conv3
I1115 10:57:03.349258 18097 net.cpp:408] conv4 -> conv4
I1115 10:57:03.364897 18097 net.cpp:150] Setting up conv4
I1115 10:57:03.364923 18097 net.cpp:157] Top shape: 200 384 13 13 (12979200)
I1115 10:57:03.364926 18097 net.cpp:165] Memory required for data: 1223856000
I1115 10:57:03.364933 18097 layer_factory.hpp:77] Creating layer relu4
I1115 10:57:03.364943 18097 net.cpp:100] Creating Layer relu4
I1115 10:57:03.364945 18097 net.cpp:434] relu4 <- conv4
I1115 10:57:03.364950 18097 net.cpp:395] relu4 -> conv4 (in-place)
I1115 10:57:03.364959 18097 net.cpp:150] Setting up relu4
I1115 10:57:03.364964 18097 net.cpp:157] Top shape: 200 384 13 13 (12979200)
I1115 10:57:03.364967 18097 net.cpp:165] Memory required for data: 1275772800
I1115 10:57:03.364970 18097 layer_factory.hpp:77] Creating layer conv5
I1115 10:57:03.364997 18097 net.cpp:100] Creating Layer conv5
I1115 10:57:03.365000 18097 net.cpp:434] conv5 <- conv4
I1115 10:57:03.365005 18097 net.cpp:408] conv5 -> conv5
I1115 10:57:03.375555 18097 net.cpp:150] Setting up conv5
I1115 10:57:03.375579 18097 net.cpp:157] Top shape: 200 256 13 13 (8652800)
I1115 10:57:03.375584 18097 net.cpp:165] Memory required for data: 1310384000
I1115 10:57:03.375596 18097 layer_factory.hpp:77] Creating layer relu5
I1115 10:57:03.375603 18097 net.cpp:100] Creating Layer relu5
I1115 10:57:03.375607 18097 net.cpp:434] relu5 <- conv5
I1115 10:57:03.375612 18097 net.cpp:395] relu5 -> conv5 (in-place)
I1115 10:57:03.375620 18097 net.cpp:150] Setting up relu5
I1115 10:57:03.375627 18097 net.cpp:157] Top shape: 200 256 13 13 (8652800)
I1115 10:57:03.375629 18097 net.cpp:165] Memory required for data: 1344995200
I1115 10:57:03.375632 18097 layer_factory.hpp:77] Creating layer pool5
I1115 10:57:03.375636 18097 net.cpp:100] Creating Layer pool5
I1115 10:57:03.375639 18097 net.cpp:434] pool5 <- conv5
I1115 10:57:03.375643 18097 net.cpp:408] pool5 -> pool5
I1115 10:57:03.375672 18097 net.cpp:150] Setting up pool5
I1115 10:57:03.375679 18097 net.cpp:157] Top shape: 200 256 6 6 (1843200)
I1115 10:57:03.375681 18097 net.cpp:165] Memory required for data: 1352368000
I1115 10:57:03.375684 18097 layer_factory.hpp:77] Creating layer fc6
I1115 10:57:03.375691 18097 net.cpp:100] Creating Layer fc6
I1115 10:57:03.375696 18097 net.cpp:434] fc6 <- pool5
I1115 10:57:03.375700 18097 net.cpp:408] fc6 -> fc6
I1115 10:57:04.236675 18097 net.cpp:150] Setting up fc6
I1115 10:57:04.236701 18097 net.cpp:157] Top shape: 200 4096 (819200)
I1115 10:57:04.236704 18097 net.cpp:165] Memory required for data: 1355644800
I1115 10:57:04.236713 18097 layer_factory.hpp:77] Creating layer relu6
I1115 10:57:04.236722 18097 net.cpp:100] Creating Layer relu6
I1115 10:57:04.236726 18097 net.cpp:434] relu6 <- fc6
I1115 10:57:04.236730 18097 net.cpp:395] relu6 -> fc6 (in-place)
I1115 10:57:04.236738 18097 net.cpp:150] Setting up relu6
I1115 10:57:04.236742 18097 net.cpp:157] Top shape: 200 4096 (819200)
I1115 10:57:04.236743 18097 net.cpp:165] Memory required for data: 1358921600
I1115 10:57:04.236747 18097 layer_factory.hpp:77] Creating layer drop6
I1115 10:57:04.236752 18097 net.cpp:100] Creating Layer drop6
I1115 10:57:04.236753 18097 net.cpp:434] drop6 <- fc6
I1115 10:57:04.236757 18097 net.cpp:395] drop6 -> fc6 (in-place)
I1115 10:57:04.236778 18097 net.cpp:150] Setting up drop6
I1115 10:57:04.236785 18097 net.cpp:157] Top shape: 200 4096 (819200)
I1115 10:57:04.236788 18097 net.cpp:165] Memory required for data: 1362198400
I1115 10:57:04.236790 18097 layer_factory.hpp:77] Creating layer fc7
I1115 10:57:04.236796 18097 net.cpp:100] Creating Layer fc7
I1115 10:57:04.236799 18097 net.cpp:434] fc7 <- fc6
I1115 10:57:04.236804 18097 net.cpp:408] fc7 -> fc7
I1115 10:57:04.617897 18097 net.cpp:150] Setting up fc7
I1115 10:57:04.617926 18097 net.cpp:157] Top shape: 200 4096 (819200)
I1115 10:57:04.617929 18097 net.cpp:165] Memory required for data: 1365475200
I1115 10:57:04.617938 18097 layer_factory.hpp:77] Creating layer relu7
I1115 10:57:04.617945 18097 net.cpp:100] Creating Layer relu7
I1115 10:57:04.617949 18097 net.cpp:434] relu7 <- fc7
I1115 10:57:04.617954 18097 net.cpp:395] relu7 -> fc7 (in-place)
I1115 10:57:04.617962 18097 net.cpp:150] Setting up relu7
I1115 10:57:04.617965 18097 net.cpp:157] Top shape: 200 4096 (819200)
I1115 10:57:04.617967 18097 net.cpp:165] Memory required for data: 1368752000
I1115 10:57:04.617969 18097 layer_factory.hpp:77] Creating layer drop7
I1115 10:57:04.617974 18097 net.cpp:100] Creating Layer drop7
I1115 10:57:04.617977 18097 net.cpp:434] drop7 <- fc7
I1115 10:57:04.617980 18097 net.cpp:395] drop7 -> fc7 (in-place)
I1115 10:57:04.618000 18097 net.cpp:150] Setting up drop7
I1115 10:57:04.618006 18097 net.cpp:157] Top shape: 200 4096 (819200)
I1115 10:57:04.618013 18097 net.cpp:165] Memory required for data: 1372028800
I1115 10:57:04.618016 18097 layer_factory.hpp:77] Creating layer fc8
I1115 10:57:04.618042 18097 net.cpp:100] Creating Layer fc8
I1115 10:57:04.618046 18097 net.cpp:434] fc8 <- fc7
I1115 10:57:04.618049 18097 net.cpp:408] fc8 -> fc8
I1115 10:57:04.711627 18097 net.cpp:150] Setting up fc8
I1115 10:57:04.711655 18097 net.cpp:157] Top shape: 200 1000 (200000)
I1115 10:57:04.711659 18097 net.cpp:165] Memory required for data: 1372828800
I1115 10:57:04.711666 18097 layer_factory.hpp:77] Creating layer loss
I1115 10:57:04.711675 18097 net.cpp:100] Creating Layer loss
I1115 10:57:04.711679 18097 net.cpp:434] loss <- fc8
I1115 10:57:04.711683 18097 net.cpp:434] loss <- label
I1115 10:57:04.711689 18097 net.cpp:408] loss -> loss
I1115 10:57:04.711704 18097 layer_factory.hpp:77] Creating layer loss
I1115 10:57:04.712290 18097 net.cpp:150] Setting up loss
I1115 10:57:04.712301 18097 net.cpp:157] Top shape: (1)
I1115 10:57:04.712304 18097 net.cpp:160]     with loss weight 1
I1115 10:57:04.712317 18097 net.cpp:165] Memory required for data: 1372828804
I1115 10:57:04.712321 18097 net.cpp:226] loss needs backward computation.
I1115 10:57:04.712323 18097 net.cpp:226] fc8 needs backward computation.
I1115 10:57:04.712327 18097 net.cpp:226] drop7 needs backward computation.
I1115 10:57:04.712328 18097 net.cpp:226] relu7 needs backward computation.
I1115 10:57:04.712332 18097 net.cpp:226] fc7 needs backward computation.
I1115 10:57:04.712333 18097 net.cpp:226] drop6 needs backward computation.
I1115 10:57:04.712335 18097 net.cpp:226] relu6 needs backward computation.
I1115 10:57:04.712337 18097 net.cpp:226] fc6 needs backward computation.
I1115 10:57:04.712340 18097 net.cpp:226] pool5 needs backward computation.
I1115 10:57:04.712343 18097 net.cpp:226] relu5 needs backward computation.
I1115 10:57:04.712345 18097 net.cpp:226] conv5 needs backward computation.
I1115 10:57:04.712348 18097 net.cpp:226] relu4 needs backward computation.
I1115 10:57:04.712350 18097 net.cpp:226] conv4 needs backward computation.
I1115 10:57:04.712353 18097 net.cpp:226] relu3 needs backward computation.
I1115 10:57:04.712355 18097 net.cpp:226] conv3 needs backward computation.
I1115 10:57:04.712358 18097 net.cpp:226] norm2 needs backward computation.
I1115 10:57:04.712360 18097 net.cpp:226] pool2 needs backward computation.
I1115 10:57:04.712363 18097 net.cpp:226] relu2 needs backward computation.
I1115 10:57:04.712366 18097 net.cpp:226] conv2 needs backward computation.
I1115 10:57:04.712368 18097 net.cpp:226] norm1 needs backward computation.
I1115 10:57:04.712371 18097 net.cpp:226] pool1 needs backward computation.
I1115 10:57:04.712374 18097 net.cpp:226] relu1 needs backward computation.
I1115 10:57:04.712376 18097 net.cpp:226] conv1 needs backward computation.
I1115 10:57:04.712379 18097 net.cpp:228] data does not need backward computation.
I1115 10:57:04.712381 18097 net.cpp:270] This network produces output loss
I1115 10:57:04.712393 18097 net.cpp:283] Network initialization done.
I1115 10:57:04.712877 18097 solver.cpp:181] Creating test net (#0) specified by net file: examples/alexFineTuning/train_val.prototxt
I1115 10:57:04.712915 18097 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1115 10:57:04.713109 18097 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "examples/alexFineTuning/mean.binaryproto"
  }
  data_param {
    source: "examples/alexFineTuning/img_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1115 10:57:04.713210 18097 layer_factory.hpp:77] Creating layer data
I1115 10:57:04.713304 18097 net.cpp:100] Creating Layer data
I1115 10:57:04.713312 18097 net.cpp:408] data -> data
I1115 10:57:04.713320 18097 net.cpp:408] data -> label
I1115 10:57:04.713328 18097 data_transformer.cpp:25] Loading mean file from: examples/alexFineTuning/mean.binaryproto
I1115 10:57:04.714778 18104 db_lmdb.cpp:35] Opened lmdb examples/alexFineTuning/img_test_lmdb
I1115 10:57:04.715113 18097 data_layer.cpp:41] output data size: 100,3,227,227
I1115 10:57:04.798682 18097 net.cpp:150] Setting up data
I1115 10:57:04.798712 18097 net.cpp:157] Top shape: 100 3 227 227 (15458700)
I1115 10:57:04.798717 18097 net.cpp:157] Top shape: 100 (100)
I1115 10:57:04.798718 18097 net.cpp:165] Memory required for data: 61835200
I1115 10:57:04.798724 18097 layer_factory.hpp:77] Creating layer label_data_1_split
I1115 10:57:04.798735 18097 net.cpp:100] Creating Layer label_data_1_split
I1115 10:57:04.798740 18097 net.cpp:434] label_data_1_split <- label
I1115 10:57:04.798746 18097 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1115 10:57:04.798756 18097 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1115 10:57:04.798811 18097 net.cpp:150] Setting up label_data_1_split
I1115 10:57:04.798818 18097 net.cpp:157] Top shape: 100 (100)
I1115 10:57:04.798821 18097 net.cpp:157] Top shape: 100 (100)
I1115 10:57:04.798823 18097 net.cpp:165] Memory required for data: 61836000
I1115 10:57:04.798826 18097 layer_factory.hpp:77] Creating layer conv1
I1115 10:57:04.798840 18097 net.cpp:100] Creating Layer conv1
I1115 10:57:04.798844 18097 net.cpp:434] conv1 <- data
I1115 10:57:04.798848 18097 net.cpp:408] conv1 -> conv1
I1115 10:57:04.799904 18097 net.cpp:150] Setting up conv1
I1115 10:57:04.799914 18097 net.cpp:157] Top shape: 100 96 55 55 (29040000)
I1115 10:57:04.799917 18097 net.cpp:165] Memory required for data: 177996000
I1115 10:57:04.799926 18097 layer_factory.hpp:77] Creating layer relu1
I1115 10:57:04.799932 18097 net.cpp:100] Creating Layer relu1
I1115 10:57:04.799937 18097 net.cpp:434] relu1 <- conv1
I1115 10:57:04.799940 18097 net.cpp:395] relu1 -> conv1 (in-place)
I1115 10:57:04.799948 18097 net.cpp:150] Setting up relu1
I1115 10:57:04.799953 18097 net.cpp:157] Top shape: 100 96 55 55 (29040000)
I1115 10:57:04.799955 18097 net.cpp:165] Memory required for data: 294156000
I1115 10:57:04.799957 18097 layer_factory.hpp:77] Creating layer pool1
I1115 10:57:04.799963 18097 net.cpp:100] Creating Layer pool1
I1115 10:57:04.799968 18097 net.cpp:434] pool1 <- conv1
I1115 10:57:04.799971 18097 net.cpp:408] pool1 -> pool1
I1115 10:57:04.809384 18097 net.cpp:150] Setting up pool1
I1115 10:57:04.809403 18097 net.cpp:157] Top shape: 100 96 27 27 (6998400)
I1115 10:57:04.809406 18097 net.cpp:165] Memory required for data: 322149600
I1115 10:57:04.809412 18097 layer_factory.hpp:77] Creating layer norm1
I1115 10:57:04.809420 18097 net.cpp:100] Creating Layer norm1
I1115 10:57:04.809425 18097 net.cpp:434] norm1 <- pool1
I1115 10:57:04.809432 18097 net.cpp:408] norm1 -> norm1
I1115 10:57:04.809460 18097 net.cpp:150] Setting up norm1
I1115 10:57:04.809468 18097 net.cpp:157] Top shape: 100 96 27 27 (6998400)
I1115 10:57:04.809469 18097 net.cpp:165] Memory required for data: 350143200
I1115 10:57:04.809473 18097 layer_factory.hpp:77] Creating layer conv2
I1115 10:57:04.809481 18097 net.cpp:100] Creating Layer conv2
I1115 10:57:04.809485 18097 net.cpp:434] conv2 <- norm1
I1115 10:57:04.809490 18097 net.cpp:408] conv2 -> conv2
I1115 10:57:04.816829 18097 net.cpp:150] Setting up conv2
I1115 10:57:04.816859 18097 net.cpp:157] Top shape: 100 256 27 27 (18662400)
I1115 10:57:04.816862 18097 net.cpp:165] Memory required for data: 424792800
I1115 10:57:04.816872 18097 layer_factory.hpp:77] Creating layer relu2
I1115 10:57:04.816881 18097 net.cpp:100] Creating Layer relu2
I1115 10:57:04.816886 18097 net.cpp:434] relu2 <- conv2
I1115 10:57:04.816891 18097 net.cpp:395] relu2 -> conv2 (in-place)
I1115 10:57:04.816916 18097 net.cpp:150] Setting up relu2
I1115 10:57:04.816922 18097 net.cpp:157] Top shape: 100 256 27 27 (18662400)
I1115 10:57:04.816925 18097 net.cpp:165] Memory required for data: 499442400
I1115 10:57:04.816927 18097 layer_factory.hpp:77] Creating layer pool2
I1115 10:57:04.816934 18097 net.cpp:100] Creating Layer pool2
I1115 10:57:04.816937 18097 net.cpp:434] pool2 <- conv2
I1115 10:57:04.816941 18097 net.cpp:408] pool2 -> pool2
I1115 10:57:04.816975 18097 net.cpp:150] Setting up pool2
I1115 10:57:04.816982 18097 net.cpp:157] Top shape: 100 256 13 13 (4326400)
I1115 10:57:04.816985 18097 net.cpp:165] Memory required for data: 516748000
I1115 10:57:04.816987 18097 layer_factory.hpp:77] Creating layer norm2
I1115 10:57:04.816992 18097 net.cpp:100] Creating Layer norm2
I1115 10:57:04.816997 18097 net.cpp:434] norm2 <- pool2
I1115 10:57:04.817000 18097 net.cpp:408] norm2 -> norm2
I1115 10:57:04.817023 18097 net.cpp:150] Setting up norm2
I1115 10:57:04.817028 18097 net.cpp:157] Top shape: 100 256 13 13 (4326400)
I1115 10:57:04.817031 18097 net.cpp:165] Memory required for data: 534053600
I1115 10:57:04.817034 18097 layer_factory.hpp:77] Creating layer conv3
I1115 10:57:04.817041 18097 net.cpp:100] Creating Layer conv3
I1115 10:57:04.817045 18097 net.cpp:434] conv3 <- norm2
I1115 10:57:04.817049 18097 net.cpp:408] conv3 -> conv3
I1115 10:57:04.837493 18097 net.cpp:150] Setting up conv3
I1115 10:57:04.837518 18097 net.cpp:157] Top shape: 100 384 13 13 (6489600)
I1115 10:57:04.837522 18097 net.cpp:165] Memory required for data: 560012000
I1115 10:57:04.837532 18097 layer_factory.hpp:77] Creating layer relu3
I1115 10:57:04.837540 18097 net.cpp:100] Creating Layer relu3
I1115 10:57:04.837545 18097 net.cpp:434] relu3 <- conv3
I1115 10:57:04.837550 18097 net.cpp:395] relu3 -> conv3 (in-place)
I1115 10:57:04.837559 18097 net.cpp:150] Setting up relu3
I1115 10:57:04.837564 18097 net.cpp:157] Top shape: 100 384 13 13 (6489600)
I1115 10:57:04.837568 18097 net.cpp:165] Memory required for data: 585970400
I1115 10:57:04.837569 18097 layer_factory.hpp:77] Creating layer conv4
I1115 10:57:04.837577 18097 net.cpp:100] Creating Layer conv4
I1115 10:57:04.837581 18097 net.cpp:434] conv4 <- conv3
I1115 10:57:04.837585 18097 net.cpp:408] conv4 -> conv4
I1115 10:57:04.856570 18097 net.cpp:150] Setting up conv4
I1115 10:57:04.856585 18097 net.cpp:157] Top shape: 100 384 13 13 (6489600)
I1115 10:57:04.856588 18097 net.cpp:165] Memory required for data: 611928800
I1115 10:57:04.856595 18097 layer_factory.hpp:77] Creating layer relu4
I1115 10:57:04.856600 18097 net.cpp:100] Creating Layer relu4
I1115 10:57:04.856606 18097 net.cpp:434] relu4 <- conv4
I1115 10:57:04.856609 18097 net.cpp:395] relu4 -> conv4 (in-place)
I1115 10:57:04.856617 18097 net.cpp:150] Setting up relu4
I1115 10:57:04.856623 18097 net.cpp:157] Top shape: 100 384 13 13 (6489600)
I1115 10:57:04.856631 18097 net.cpp:165] Memory required for data: 637887200
I1115 10:57:04.856634 18097 layer_factory.hpp:77] Creating layer conv5
I1115 10:57:04.856644 18097 net.cpp:100] Creating Layer conv5
I1115 10:57:04.856648 18097 net.cpp:434] conv5 <- conv4
I1115 10:57:04.856652 18097 net.cpp:408] conv5 -> conv5
I1115 10:57:04.867108 18097 net.cpp:150] Setting up conv5
I1115 10:57:04.867136 18097 net.cpp:157] Top shape: 100 256 13 13 (4326400)
I1115 10:57:04.867138 18097 net.cpp:165] Memory required for data: 655192800
I1115 10:57:04.867149 18097 layer_factory.hpp:77] Creating layer relu5
I1115 10:57:04.867158 18097 net.cpp:100] Creating Layer relu5
I1115 10:57:04.867163 18097 net.cpp:434] relu5 <- conv5
I1115 10:57:04.867168 18097 net.cpp:395] relu5 -> conv5 (in-place)
I1115 10:57:04.867177 18097 net.cpp:150] Setting up relu5
I1115 10:57:04.867182 18097 net.cpp:157] Top shape: 100 256 13 13 (4326400)
I1115 10:57:04.867184 18097 net.cpp:165] Memory required for data: 672498400
I1115 10:57:04.867187 18097 layer_factory.hpp:77] Creating layer pool5
I1115 10:57:04.867194 18097 net.cpp:100] Creating Layer pool5
I1115 10:57:04.867198 18097 net.cpp:434] pool5 <- conv5
I1115 10:57:04.867219 18097 net.cpp:408] pool5 -> pool5
I1115 10:57:04.867254 18097 net.cpp:150] Setting up pool5
I1115 10:57:04.867260 18097 net.cpp:157] Top shape: 100 256 6 6 (921600)
I1115 10:57:04.867264 18097 net.cpp:165] Memory required for data: 676184800
I1115 10:57:04.867265 18097 layer_factory.hpp:77] Creating layer fc6
I1115 10:57:04.867272 18097 net.cpp:100] Creating Layer fc6
I1115 10:57:04.867276 18097 net.cpp:434] fc6 <- pool5
I1115 10:57:04.867280 18097 net.cpp:408] fc6 -> fc6
I1115 10:57:05.720427 18097 net.cpp:150] Setting up fc6
I1115 10:57:05.720455 18097 net.cpp:157] Top shape: 100 4096 (409600)
I1115 10:57:05.720459 18097 net.cpp:165] Memory required for data: 677823200
I1115 10:57:05.720468 18097 layer_factory.hpp:77] Creating layer relu6
I1115 10:57:05.720479 18097 net.cpp:100] Creating Layer relu6
I1115 10:57:05.720484 18097 net.cpp:434] relu6 <- fc6
I1115 10:57:05.720491 18097 net.cpp:395] relu6 -> fc6 (in-place)
I1115 10:57:05.720501 18097 net.cpp:150] Setting up relu6
I1115 10:57:05.720506 18097 net.cpp:157] Top shape: 100 4096 (409600)
I1115 10:57:05.720510 18097 net.cpp:165] Memory required for data: 679461600
I1115 10:57:05.720512 18097 layer_factory.hpp:77] Creating layer drop6
I1115 10:57:05.720520 18097 net.cpp:100] Creating Layer drop6
I1115 10:57:05.720523 18097 net.cpp:434] drop6 <- fc6
I1115 10:57:05.720527 18097 net.cpp:395] drop6 -> fc6 (in-place)
I1115 10:57:05.720548 18097 net.cpp:150] Setting up drop6
I1115 10:57:05.720554 18097 net.cpp:157] Top shape: 100 4096 (409600)
I1115 10:57:05.720558 18097 net.cpp:165] Memory required for data: 681100000
I1115 10:57:05.720561 18097 layer_factory.hpp:77] Creating layer fc7
I1115 10:57:05.720567 18097 net.cpp:100] Creating Layer fc7
I1115 10:57:05.720571 18097 net.cpp:434] fc7 <- fc6
I1115 10:57:05.720576 18097 net.cpp:408] fc7 -> fc7
I1115 10:57:06.098704 18097 net.cpp:150] Setting up fc7
I1115 10:57:06.098731 18097 net.cpp:157] Top shape: 100 4096 (409600)
I1115 10:57:06.098733 18097 net.cpp:165] Memory required for data: 682738400
I1115 10:57:06.098742 18097 layer_factory.hpp:77] Creating layer relu7
I1115 10:57:06.098752 18097 net.cpp:100] Creating Layer relu7
I1115 10:57:06.098757 18097 net.cpp:434] relu7 <- fc7
I1115 10:57:06.098765 18097 net.cpp:395] relu7 -> fc7 (in-place)
I1115 10:57:06.098775 18097 net.cpp:150] Setting up relu7
I1115 10:57:06.098780 18097 net.cpp:157] Top shape: 100 4096 (409600)
I1115 10:57:06.098784 18097 net.cpp:165] Memory required for data: 684376800
I1115 10:57:06.098786 18097 layer_factory.hpp:77] Creating layer drop7
I1115 10:57:06.098794 18097 net.cpp:100] Creating Layer drop7
I1115 10:57:06.098798 18097 net.cpp:434] drop7 <- fc7
I1115 10:57:06.098801 18097 net.cpp:395] drop7 -> fc7 (in-place)
I1115 10:57:06.098824 18097 net.cpp:150] Setting up drop7
I1115 10:57:06.098830 18097 net.cpp:157] Top shape: 100 4096 (409600)
I1115 10:57:06.098834 18097 net.cpp:165] Memory required for data: 686015200
I1115 10:57:06.098836 18097 layer_factory.hpp:77] Creating layer fc8
I1115 10:57:06.098845 18097 net.cpp:100] Creating Layer fc8
I1115 10:57:06.098848 18097 net.cpp:434] fc8 <- fc7
I1115 10:57:06.098852 18097 net.cpp:408] fc8 -> fc8
I1115 10:57:06.191231 18097 net.cpp:150] Setting up fc8
I1115 10:57:06.191262 18097 net.cpp:157] Top shape: 100 1000 (100000)
I1115 10:57:06.191264 18097 net.cpp:165] Memory required for data: 686415200
I1115 10:57:06.191272 18097 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1115 10:57:06.191282 18097 net.cpp:100] Creating Layer fc8_fc8_0_split
I1115 10:57:06.191287 18097 net.cpp:434] fc8_fc8_0_split <- fc8
I1115 10:57:06.191294 18097 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1115 10:57:06.191304 18097 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1115 10:57:06.191335 18097 net.cpp:150] Setting up fc8_fc8_0_split
I1115 10:57:06.191342 18097 net.cpp:157] Top shape: 100 1000 (100000)
I1115 10:57:06.191346 18097 net.cpp:157] Top shape: 100 1000 (100000)
I1115 10:57:06.191349 18097 net.cpp:165] Memory required for data: 687215200
I1115 10:57:06.191354 18097 layer_factory.hpp:77] Creating layer accuracy
I1115 10:57:06.191383 18097 net.cpp:100] Creating Layer accuracy
I1115 10:57:06.191388 18097 net.cpp:434] accuracy <- fc8_fc8_0_split_0
I1115 10:57:06.191392 18097 net.cpp:434] accuracy <- label_data_1_split_0
I1115 10:57:06.191397 18097 net.cpp:408] accuracy -> accuracy
I1115 10:57:06.191406 18097 net.cpp:150] Setting up accuracy
I1115 10:57:06.191411 18097 net.cpp:157] Top shape: (1)
I1115 10:57:06.191416 18097 net.cpp:165] Memory required for data: 687215204
I1115 10:57:06.191417 18097 layer_factory.hpp:77] Creating layer loss
I1115 10:57:06.191423 18097 net.cpp:100] Creating Layer loss
I1115 10:57:06.191427 18097 net.cpp:434] loss <- fc8_fc8_0_split_1
I1115 10:57:06.191431 18097 net.cpp:434] loss <- label_data_1_split_1
I1115 10:57:06.191435 18097 net.cpp:408] loss -> loss
I1115 10:57:06.191443 18097 layer_factory.hpp:77] Creating layer loss
I1115 10:57:06.191602 18097 net.cpp:150] Setting up loss
I1115 10:57:06.191612 18097 net.cpp:157] Top shape: (1)
I1115 10:57:06.191627 18097 net.cpp:160]     with loss weight 1
I1115 10:57:06.191639 18097 net.cpp:165] Memory required for data: 687215208
I1115 10:57:06.191642 18097 net.cpp:226] loss needs backward computation.
I1115 10:57:06.191649 18097 net.cpp:228] accuracy does not need backward computation.
I1115 10:57:06.191653 18097 net.cpp:226] fc8_fc8_0_split needs backward computation.
I1115 10:57:06.191658 18097 net.cpp:226] fc8 needs backward computation.
I1115 10:57:06.191660 18097 net.cpp:226] drop7 needs backward computation.
I1115 10:57:06.191664 18097 net.cpp:226] relu7 needs backward computation.
I1115 10:57:06.191668 18097 net.cpp:226] fc7 needs backward computation.
I1115 10:57:06.191680 18097 net.cpp:226] drop6 needs backward computation.
I1115 10:57:06.191685 18097 net.cpp:226] relu6 needs backward computation.
I1115 10:57:06.191687 18097 net.cpp:226] fc6 needs backward computation.
I1115 10:57:06.191692 18097 net.cpp:226] pool5 needs backward computation.
I1115 10:57:06.191695 18097 net.cpp:226] relu5 needs backward computation.
I1115 10:57:06.191699 18097 net.cpp:226] conv5 needs backward computation.
I1115 10:57:06.191701 18097 net.cpp:226] relu4 needs backward computation.
I1115 10:57:06.191705 18097 net.cpp:226] conv4 needs backward computation.
I1115 10:57:06.191709 18097 net.cpp:226] relu3 needs backward computation.
I1115 10:57:06.191711 18097 net.cpp:226] conv3 needs backward computation.
I1115 10:57:06.191715 18097 net.cpp:226] norm2 needs backward computation.
I1115 10:57:06.191718 18097 net.cpp:226] pool2 needs backward computation.
I1115 10:57:06.191726 18097 net.cpp:226] relu2 needs backward computation.
I1115 10:57:06.191730 18097 net.cpp:226] conv2 needs backward computation.
I1115 10:57:06.191733 18097 net.cpp:226] norm1 needs backward computation.
I1115 10:57:06.191737 18097 net.cpp:226] pool1 needs backward computation.
I1115 10:57:06.191740 18097 net.cpp:226] relu1 needs backward computation.
I1115 10:57:06.191745 18097 net.cpp:226] conv1 needs backward computation.
I1115 10:57:06.191747 18097 net.cpp:228] label_data_1_split does not need backward computation.
I1115 10:57:06.191752 18097 net.cpp:228] data does not need backward computation.
I1115 10:57:06.191756 18097 net.cpp:270] This network produces output accuracy
I1115 10:57:06.191758 18097 net.cpp:270] This network produces output loss
I1115 10:57:06.191777 18097 net.cpp:283] Network initialization done.
I1115 10:57:06.191870 18097 solver.cpp:60] Solver scaffolding done.
I1115 10:57:06.192343 18097 caffe.cpp:155] Finetuning from examples/alexFineTuning/bvlc_reference_caffenet.caffemodel
I1115 10:57:06.633036 18097 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: examples/alexFineTuning/bvlc_reference_caffenet.caffemodel
I1115 10:57:06.633062 18097 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1115 10:57:06.633066 18097 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1115 10:57:06.633168 18097 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: examples/alexFineTuning/bvlc_reference_caffenet.caffemodel
I1115 10:57:07.505892 18097 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1115 10:57:08.002562 18097 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: examples/alexFineTuning/bvlc_reference_caffenet.caffemodel
I1115 10:57:08.002588 18097 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1115 10:57:08.002602 18097 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1115 10:57:08.002616 18097 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: examples/alexFineTuning/bvlc_reference_caffenet.caffemodel
I1115 10:57:08.880524 18097 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1115 10:57:08.935586 18097 caffe.cpp:251] Starting Optimization
I1115 10:57:08.935626 18097 solver.cpp:279] Solving CaffeNet
I1115 10:57:08.935629 18097 solver.cpp:280] Learning Rate Policy: step
I1115 10:57:08.937263 18097 solver.cpp:337] Iteration 0, Testing net (#0)
I1115 10:57:14.427781 18097 solver.cpp:404]     Test net output #0: accuracy = 0
I1115 10:57:14.427812 18097 solver.cpp:404]     Test net output #1: loss = 9.89052 (* 1 = 9.89052 loss)
I1115 10:57:15.187870 18097 solver.cpp:228] Iteration 0, loss = 10.5626
I1115 10:57:15.187913 18097 solver.cpp:244]     Train net output #0: loss = 10.5626 (* 1 = 10.5626 loss)
I1115 10:57:15.187925 18097 sgd_solver.cpp:106] Iteration 0, lr = 0.005
I1115 11:00:02.386097 18097 solver.cpp:337] Iteration 200, Testing net (#0)
I1115 11:00:07.924772 18097 solver.cpp:404]     Test net output #0: accuracy = 0
I1115 11:00:07.924798 18097 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
